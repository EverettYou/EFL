{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entanglement Feature Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EFL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convension of Coupling Constants\n",
    "The original EFL model (for the Ising case) is\n",
    "$$E[\\sigma]=-\\sum_{\\langle ij\\rangle}J_{ij} \\chi(\\sigma_i^{-1}\\sigma_j),$$\n",
    "where $J_{ij}=\\ln D_{ij}$ and $\\sigma_{i}$ takes values in the $S_2$ group, and the cycle trace $\\chi$ maps $()\\to2$, $(1,2)\\to1$. The energy difference is one unit of $J_{ij}$. Note that each bond $\\langle ij\\rangle$ is added only once in the summation. Now for easier treatment in numerics, we map the $S_2$ spin $\\sigma_i$ to a $\\mathbb{Z}_2$ spin $s_i$, s.t. the energy model becomes\n",
    "$$E[s]=-\\frac{1}{2}\\sum_{\\langle ij\\rangle}J_{ij}s_is_j=-\\frac{1}{4}\\sum_{i j}J_{ij}s_is_j.$$\n",
    "We then define\n",
    "$$K_{ij}=\\frac{1}{2}J_{ij}=\\frac{1}{2}\\ln D_{ij},$$\n",
    "and rewrite the energy model as\n",
    "$$E[s]=-\\frac{1}{2}\\sum_{ij}K_{ij}s_is_j,$$\n",
    "where the summation double counts a bond, and $s_{i}=\\pm1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Boltzmann Machine (Model)\n",
    "\n",
    "Layers: $s^{0},s^{1},\\cdots$, where $s^0$ is the visible layer, rests are hidden layers. The energy model:\n",
    "$$E[s]=-\\sum_{l=1:L}\\sum_{i j}s_i^{l-1}K_{ij}^l s_j^l.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzman Machine (RBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBM Kernel\n",
    "Differences to standard RBM:\n",
    "- the binary units takes values $\\pm1$ instead of $0,1$.\n",
    "- **unbiased**, RTN of pure state are not biased, so the only variational parameter is the weight matrix.\n",
    "- elements in the weight matrixes are **positive** definite, because they correspnd to the logarithmic bond dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of weight matrix. Wish to:\n",
    "- reflect the locality.\n",
    "- ferromagnetic, tune to critical.\n",
    "- with some randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Gibbs sampling:\n",
    "- propagating up:\n",
    "$$h_j\\leftarrow\\tanh \\left\\{\\begin{array}{cc}\\sum_{i}W_{ij}v_i & \\text{default, top},\\\\ 2\\sum_{i}W_{ij}v_i & \\text{bottom, intermediate},\\end{array}\\right.$$\n",
    "- propagating down:\n",
    "$$v_i\\leftarrow\\tanh \\left\\{\\begin{array}{cc}\\sum_{j}W_{ij}h_j & \\text{default, bottom},\\\\ 2\\sum_{j}W_{ij}h_j & \\text{top, intermediate},\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update rule ($\\lambda_\\text{l}$ learning rate, $\\lambda_\\text{f}$ forgetting rate):\n",
    "$$W\\leftarrow \\text{relu}[(1-\\lambda_\\text{f})W+\\lambda_\\text{l} (v^{\\intercal}(0) h(0)-v^{\\intercal}(\\infty) h(\\infty))]$$\n",
    "- If any element in $W$ becomes negative, the element is set to zero.\n",
    "- Forgetting rate can be used to control the bond dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run 'EFL.py'\n",
    "rbm = RBM(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7453134201113543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.165,  0.011],\n",
       "       [ 0.708,  0.176],\n",
       "       [ 0.120,  0.285],\n",
       "       [ 0.071,  0.809]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rbm.learn(numpy.asarray(numpy.random.randint(0,2,(40,4))*2-1,dtype=float),0.1,0.01))\n",
    "rbm.W.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.081018265501512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.194,  0.031],\n",
       "       [ 0.005,  0.940],\n",
       "       [ 1.189,  0.003],\n",
       "       [ 0.000,  0.946]])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rbm.learn(numpy.array([[1,1,1,1],[-1,1,-1,1]]*10,dtype=float),0.1,0.01))\n",
    "rbm.W.get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test by Ideal States\n",
    "* trivial product state: deep PM, all Ising configuration equal weight.\n",
    "* random state (maximally thermalized): deep FM, all up or all down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run EFL.py\n",
    "train_FM = numpy.array([[1,1,1,1],[-1,-1,-1,-1]]*200,dtype=float)\n",
    "train_AFM = numpy.array([[1,-1,1,-1],[-1,1,-1,1]]*200,dtype=float)\n",
    "train_PM = numpy.asarray(numpy.random.randint(0,2,(400,4))*2-1,dtype=float)\n",
    "#lr_table = [0.2,0.3,0.25,0.15,0.1]\n",
    "#fr_table = [0.05,0.01,0.,0.,0.]\n",
    "lr_table = [0.5,0.3,0.2,0.15,0.1,0.1,0.1]\n",
    "fr_table = [0.,0.,0.,0.,0.,0.,0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FM, maximally thermal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  cost = 0.946498\n",
      "Epoch 1:  cost = 0.901662\n",
      "Epoch 2:  cost = 0.687882\n",
      "Epoch 3:  cost = 0.599588\n",
      "Epoch 4:  cost = 0.612765\n",
      "Epoch 5:  cost = 0.556824\n",
      "Epoch 6:  cost = 0.536112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.845,  0.575],\n",
       "       [ 1.516,  0.517],\n",
       "       [ 0.737,  0.613],\n",
       "       [ 0.642,  0.933]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm = RBM(Nv=4,Nh=2)\n",
    "for epoch in range(7):\n",
    "    cost = rbm.learn(train_FM,lr_table[epoch],fr_table[epoch])\n",
    "    print('Epoch %d: '%epoch, 'cost = %f'%cost)\n",
    "rbm.W.get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PM, trivial product state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  cost = 1.026209\n",
      "Epoch 1:  cost = 1.021890\n",
      "Epoch 2:  cost = 1.005196\n",
      "Epoch 3:  cost = 0.953806\n",
      "Epoch 4:  cost = 0.973413\n",
      "Epoch 5:  cost = 0.966338\n",
      "Epoch 6:  cost = 0.966973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.397,  0.003],\n",
       "       [ 0.089,  0.006],\n",
       "       [ 0.000,  0.400],\n",
       "       [ 0.018,  0.692]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm = RBM(Nv=4,Nh=2)\n",
    "for epoch in range(7):\n",
    "    cost = rbm.learn(train_PM,lr_table[epoch],fr_table[epoch])\n",
    "    print('Epoch %d: '%epoch, 'cost = %f'%cost)\n",
    "rbm.W.get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional RBM\n",
    "Assumption: translational symmetry (or in statistical sense under disorder average). Weights are shared among kernels for each layer and each group. This makes the algorithm scalable. For disordered system, this will learn the disorder averaged entanglement features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Consider a 1D free fermion CFT, the Renyi entropy given by (acorrding to Calabrese and Cardy 2004, 2009)\n",
    "$$S\\propto\\sum_{i,j}\\ln|u_i-v_j|-\\sum_{i<j}\\ln|u_i-u_j|-\\sum_{i<j}\\ln|v_i-v_j|.$$\n",
    "$u_i$ and $v_i$ are positions of kinks and antikinks in the Ising configuraiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9459101490553132"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "def entropy_CFT(kinks):\n",
    "    us = kinks[0::2]\n",
    "    vs = kinks[1::2]\n",
    "    Suv = sum(numpy.log(abs(u-v)) for u in us for v in vs)\n",
    "    Suu = sum(numpy.log(abs(u1-u2)) for u1, u2 in combinations(us,2))\n",
    "    Svv = sum(numpy.log(abs(v1-v2)) for v1, v2 in combinations(vs,2))\n",
    "    S = Suv-Suu-Svv\n",
    "    return S\n",
    "entropy_CFT([3,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deep Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DBM and prepare training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run 'EFL.py'\n",
    "dbm = DBM([8,4,2,1])\n",
    "test_samples = [\n",
    "    [+1,+1,+1,+1,+1,+1,+1,+1],\n",
    "    [-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "    [+1,+1,+1,+1,-1,-1,-1,-1],\n",
    "    [-1,-1,-1,-1,+1,+1,+1,+1],\n",
    "    #[+1,+1,-1,-1,-1,-1,+1,+1],\n",
    "    #[-1,-1,+1,+1,+1,+1,-1,-1]\n",
    "]\n",
    "data = Server(test_samples*100,15)\n",
    "#data = Server(numpy.random.randint(0,2,(200,8))*2-1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Pretrain and show weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining layer 0:\n",
      "    Epoch 0: cost = 0.575124\n",
      "    Epoch 1: cost = 0.190124\n",
      "    Epoch 2: cost = 0.065941\n",
      "    Epoch 3: cost = 0.030259\n",
      "    Epoch 4: cost = 0.055031\n",
      "    Epoch 5: cost = 0.010158\n",
      "    Epoch 6: cost = 0.006926\n",
      "Pretraining layer 1:\n",
      "    Epoch 0: cost = 1.623854\n",
      "    Epoch 1: cost = 1.452538\n",
      "    Epoch 2: cost = 1.115676\n",
      "    Epoch 3: cost = 0.755499\n",
      "    Epoch 4: cost = 0.741680\n",
      "    Epoch 5: cost = 0.667127\n",
      "    Epoch 6: cost = 0.500334\n",
      "Pretraining layer 2:\n",
      "    Epoch 0: cost = 1.017601\n",
      "    Epoch 1: cost = 1.001798\n",
      "    Epoch 2: cost = 1.004361\n",
      "    Epoch 3: cost = 1.003599\n"
     ]
    }
   ],
   "source": [
    "dbm.pretrain(data,lrs=[0.5,0.3,0.2,0.15],frs=[0.05,0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.356  1.367  0.000  0.000]\n",
      " [ 1.330  1.335  0.021  0.020]\n",
      " [ 1.306  1.299  0.067  0.067]\n",
      " [ 1.258  1.322  0.013  0.013]\n",
      " [ 0.017  0.018  1.357  1.333]\n",
      " [ 0.027  0.027  1.415  1.351]\n",
      " [ 0.097  0.098  1.432  1.428]\n",
      " [ 0.044  0.044  1.355  1.442]]\n",
      "[[ 1.295  0.033]\n",
      " [ 1.290  0.047]\n",
      " [ 0.060  1.330]\n",
      " [ 0.033  1.318]]\n",
      "[[ 0.025]\n",
      " [ 0.014]]\n"
     ]
    }
   ],
   "source": [
    "for rbm in dbm.rbms:\n",
    "    print(rbm.W.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.000,  1.000],\n",
       "       [-1.000, -1.000],\n",
       "       [-1.000,  1.000],\n",
       "       [ 1.000, -1.000]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = theano.function([dbm.input],dbm.rbms[1].output)\n",
    "f([[+1,+1,+1,+1,+1,+1,+1,+1],\n",
    "   [-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "   [-1,-1,-1,-1,+1,+1,+1,+1],\n",
    "   [+1,+1,+1,+1,-1,-1,-1,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning and show weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 0: cost = 0.005486\n",
      "    Epoch 1: cost = 0.003521\n",
      "    Epoch 2: cost = 0.003186\n",
      "    Epoch 3: cost = 0.001988\n",
      "    Epoch 4: cost = 0.001897\n",
      "    Epoch 5: cost = 0.001822\n",
      "    Epoch 6: cost = 0.001761\n",
      "    Epoch 7: cost = 0.001747\n",
      "    Epoch 8: cost = 0.001740\n",
      "    Epoch 9: cost = 0.001734\n"
     ]
    }
   ],
   "source": [
    "dbm.finetune(data,10,lrs=[0.5,0.5,0.4,0.3,0.2,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.712  1.722  0.000  0.000]\n",
      " [ 1.647  1.652  0.000  0.000]\n",
      " [ 1.766  1.759  0.000  0.000]\n",
      " [ 1.528  1.592  0.000  0.000]\n",
      " [ 0.000  0.000  1.680  1.656]\n",
      " [ 0.000  0.000  1.755  1.691]\n",
      " [ 0.000  0.000  1.731  1.727]\n",
      " [ 0.000  0.000  1.657  1.743]]\n",
      "[[ 1.293  0.000]\n",
      " [ 1.288  0.000]\n",
      " [ 0.000  1.330]\n",
      " [ 0.000  1.318]]\n",
      "[[ 0.018]\n",
      " [ 0.000]]\n"
     ]
    }
   ],
   "source": [
    "for rbm in dbm.rbms:\n",
    "    print(rbm.W.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Heisenberg Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Heisenberg model:\n",
    "$$H=J_1\\sum_{\\langle ij\\rangle}\\vec{S}_i\\cdot\\vec{S}_j+J_2\\sum_{\\langle\\langle ij\\rangle\\rangle}(S_i^+S_j^-+h.c.)+\\sum_i h_i S_i^z,$$\n",
    "where $h_i\\in[-W,W]$ is independently random on each site. According to Vedica, the MBL-ETH transition happens around $W_c\\simeq 5$. We assume periodic boundary condition, and ED the Hamiltonian in the total-$S^z$ zero sector.\n",
    "\n",
    "`Entanglement` object takes the many-body wave function given by `random_Heisenberg`. It than perform the MPS decomposition, and construct  entanglement entropy (2nd Renyi, in unit of bit) for all entanglement regions (secified by Ising configruation). The result is stored as a dict in `Entanglement.Sdict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (-1.6017132519074577e-15, -0.1999999999999999),\n",
       " 1: (0.98503959530704377, 0.016471315244866325),\n",
       " 2: (1.9095470859390562, 0.029583699542407138),\n",
       " 3: (2.7245355665404101, 0.034817811191220994),\n",
       " 4: (3.3230399205273935, 0.040609214620450293),\n",
       " 5: (3.5502721680280094, 0.04225125503175331)}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run 'EFL.py'\n",
    "ent = Entanglement(random_Heisenberg(L = 10, W = 1.))\n",
    "d = {k:[] for k in range(0,6)}\n",
    "for config, S in ent.Sdict.items():\n",
    "    d[min(config.count(1),config.count(-1))].append(S)\n",
    "{k:(numpy.mean(Ss), numpy.std(Ss)/numpy.mean(Ss)) for k, Ss in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
